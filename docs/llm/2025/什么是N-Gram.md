---
layout: doc
title: 什么是N-Gram
date: 2024-02-15
category: llm
tags: [llm, architecture, enterprise]
excerpt: 什么是N-Gram
permalink: /docs/llm/2025/enterprise-llm-arch/
---


## 定义
N-Gram是自然语言处理（NLP）中一种基于统计语言模型的基本技术，它表示文本中连续出现的N个项目（通常是单词或字符）的序列。这里的"N"是一个正整数，可以是1、2、3等，分别对应不同类型的N-Gram：
- **1-Gram（Unigram）**：单个单词或字符，如"我"、"爱"、"自然语言"
- **2-Gram（Bigram）**：连续的两个项目，如"我爱"、"自然语言处理"
- **3-Gram（Trigram）**：连续的三个项目，如"我爱自然"、"自然语言处理技术"

## 核心原理
N-Gram模型基于马尔可夫假设，即一个词的出现仅与它前面N-1个词相关。通过统计语料库中不同N-Gram序列出现的频率，可以计算出文本序列的概率分布，从而实现语言模型的核心功能——预测下一个可能出现的词。

## 主要应用
1. **文本生成**：根据前面出现的词预测后续词语，用于自动补全、机器翻译等
2. **拼写纠错**：通过比较错误拼写与正确N-Gram序列的概率来识别和纠正拼写错误
3. **文本分类**：将文本表示为N-Gram特征向量，用于垃圾邮件检测、情感分析等
4. **语音识别**：结合声学模型，通过N-Gram语言模型提高识别准确率

## 优缺点
### 优点
- 实现简单，计算效率高
- 对训练数据的需求量相对较小
- 能够捕捉局部上下文信息
- 在小规模语料库上表现稳定

### 缺点
- 无法捕捉长距离依赖关系
- N值选择对性能影响大（N太小捕捉信息不足，N太大导致数据稀疏）
- 对未见过的N-Gram序列（OOV问题）处理能力有限

## 实际应用案例
在搜索引擎中，当用户输入"自然语言"时，搜索引擎可以利用Bigram模型预测用户可能接下来输入"处理"、"理解"或"技术"等词，从而提供更精准的搜索建议。

## 扩展概念
- **N-Gram语言模型**：基于N-Gram的概率模型，用于计算文本序列的概率
- **平滑技术**：如Add-One、Good-Turing等，用于解决数据稀疏问题
- **Skip-Gram**：Word2Vec中的一种模型，与N-Gram不同，它通过中心词预测上下文词
- **BERT**：基于Transformer的预训练模型，利用N-Gram作为输入特征